{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d98636c-9a39-457e-8443-e79dd4d6ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf90dc18-b0f8-422a-99b6-510a41f6610a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_hot(syl_word):\n",
    "    hot = []\n",
    "    i = 0\n",
    "    while  i < len(syl_word):\n",
    "        if i == len(syl_word) - 1:\n",
    "            hot += [1]\n",
    "            return hot\n",
    "        if syl_word[i+1] == '-':\n",
    "            hot += [2]\n",
    "            i += 2\n",
    "        else:\n",
    "            hot += [1]\n",
    "            i += 1\n",
    "    return hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d13b2c8e-c4dc-49ad-bc62-13e77339999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78b86d51-38fd-44a8-8117-45a3ffc43193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo¥re\n",
      "49217\n",
      "49217\n"
     ]
    }
   ],
   "source": [
    "print(orig_data[orig_data_syl.index('lo-re')])\n",
    "print(orig_data_syl.index('flo-r-al'))\n",
    "print(orig_data_eng.index('floral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03afe84a-380a-47d2-964b-26215314ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187174/187175\r"
     ]
    }
   ],
   "source": [
    "training_data_size = 10000\n",
    "e2i_vocab_ortho = []\n",
    "e2i_vocab_ipa = []\n",
    "\n",
    "max_encoder_len = 0\n",
    "\n",
    "x_tr_ortho = []\n",
    "x_tr_ipa = []\n",
    "y_tr = []\n",
    "\n",
    "x_val_ortho = []\n",
    "x_val_ipa = []\n",
    "y_val = []\n",
    "\n",
    "parser = WiktionaryParser()\n",
    "orig_file = open(\"mhyph.txt\", encoding='ISO-8859-1')\n",
    "\n",
    "orig_data = orig_file.readlines()\n",
    "orig_file.close()\n",
    "orig_data = [line.strip('\\n') for line in orig_data]\n",
    "random.shuffle(orig_data)\n",
    "orig_data_eng = [line.replace('¥', '').lower() for line in orig_data]\n",
    "orig_data_syl = [line.replace('¥', '-').lower() for line in orig_data]\n",
    "\n",
    "filtered_eng = []\n",
    "filtered_syl = []\n",
    "filtered_ipa = []\n",
    "\n",
    "max_encoder_len_ortho = len(max(orig_data_eng, key=len))\n",
    "\n",
    "for i in range(0, len(orig_data_eng)):\n",
    "    if orig_data_eng[i].isalpha():\n",
    "        try:\n",
    "            ipa = parser.fetch(orig_data_eng[i])\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            if ipa:\n",
    "                if ipa[0]['pronunciations']['text']:\n",
    "                    for e in ipa[0]['pronunciations']['text']:\n",
    "                        if 'IPA: ' in e:\n",
    "                            filtered_eng += [orig_data_eng[i]]\n",
    "                            filtered_syl += [orig_data_syl[i]]\n",
    "                            filtered_ipa += [e.split('IPA: ')[-1].split(',')[0]]\n",
    "                            break\n",
    "    print(str(i) + '/%i'%len(orig_data_eng), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0317681-0dc4-4767-ba6f-d7fc4185d9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flo-r-al\n",
      "10018\n"
     ]
    }
   ],
   "source": [
    "print(filtered_syl[filtered_syl.index('flo-r-al')])\n",
    "print(filtered_syl.index('flo-r-al'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79ee75ac-69d1-4a83-b918-831db2cfbf49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_tr_ortho = []\n",
    "x_tr_ipa = []\n",
    "\n",
    "y_tr = []\n",
    "\n",
    "for line in filtered_eng:\n",
    "    for c in line:\n",
    "        if c not in e2i_vocab_ortho:\n",
    "            e2i_vocab_ortho += [c]\n",
    "            \n",
    "for line in filtered_ipa:\n",
    "    for c in line:\n",
    "        if c not in e2i_vocab_ipa:\n",
    "            e2i_vocab_ipa += [c]\n",
    "\n",
    "e2i_ortho = dict((a,i) for i,a in enumerate(e2i_vocab_ortho, 1))\n",
    "e2i_ipa = dict((a,i) for i,a in enumerate(e2i_vocab_ipa, 1))\n",
    "\n",
    "for line in filtered_eng[:training_data_size]:\n",
    "    converted = []\n",
    "    for c in line:\n",
    "        converted += [e2i_ortho[c]]\n",
    "    x_tr_ortho += [converted]\n",
    "    \n",
    "x_tr_ortho = pad_sequences(x_tr_ortho, maxlen=max_encoder_len_ortho, padding='post')\n",
    "\n",
    "for line in filtered_ipa[:training_data_size]:\n",
    "    converted = []\n",
    "    for c in line:\n",
    "        converted += [e2i_ipa[c]]\n",
    "    x_tr_ipa += [converted]\n",
    "    \n",
    "x_tr_ipa = pad_sequences(x_tr_ipa, maxlen=max_encoder_len_ipa, padding='post')\n",
    "\n",
    "for line in filtered_syl[:training_data_size]:\n",
    "    y_tr += [convert_to_hot(line)]\n",
    "\n",
    "y_tr = pad_sequences(y_tr, maxlen=max_encoder_len_ipa, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "x_tr_file_ortho = open('data/x_tr_ortho.pkl','wb')\n",
    "pickle.dump(x_tr_ortho, x_tr_file_ortho)\n",
    "x_tr_file_ortho.close()\n",
    "\n",
    "x_tr_file_ipa = open('data/x_tr_ipa.pkl','wb')\n",
    "pickle.dump(x_tr_ipa, x_tr_file_ipa)\n",
    "x_tr_file_ipa.close()\n",
    "\n",
    "y_tr_file = open('data/y_tr.pkl','wb')\n",
    "pickle.dump(y_tr, y_tr_file)\n",
    "y_tr_file.close()\n",
    "\n",
    "e2i_ortho_file = open('data/e2i_ortho.pkl','wb')\n",
    "pickle.dump(e2i_ortho, e2i_ortho_file)\n",
    "e2i_ortho_file.close()\n",
    "\n",
    "e2i_ipa_file = open('data/e2i_ipa.pkl','wb')\n",
    "pickle.dump(e2i_ipa, e2i_ipa_file)\n",
    "e2i_ipa_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1117051-8746-485d-abf0-210ce714cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_tr_ortho))\n",
    "print(len(x_tr_ipa))\n",
    "print(len(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "606929bd-8d38-496f-ac44-6d91e4eabf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_ortho = []\n",
    "x_val_ipa = []\n",
    "y_val = []\n",
    "\n",
    "for line in filtered_eng[training_data_size:]:\n",
    "    converted = []\n",
    "    for c in line:\n",
    "        converted += [e2i_ortho[c]]\n",
    "    x_val_ortho += [converted]\n",
    "    \n",
    "x_val_ortho = pad_sequences(x_val_ortho, maxlen=max_encoder_len_ortho, padding='post')\n",
    "\n",
    "for line in filtered_ipa[training_data_size:]:\n",
    "    converted = []\n",
    "    for c in line:\n",
    "        converted += [e2i_ipa[c]]\n",
    "    x_val_ipa += [converted]\n",
    "    \n",
    "x_val_ipa = pad_sequences(x_val_ipa, maxlen=max_encoder_len_ipa, padding='post')\n",
    "\n",
    "for line in filtered_syl[training_data_size:]:\n",
    "    y_val += [convert_to_hot(line)]\n",
    "\n",
    "y_val = pad_sequences(y_val, maxlen=max_encoder_len_ipa, padding='post')\n",
    "\n",
    "\n",
    "x_val_file_ortho = open('data/x_val_ortho.pkl','wb')\n",
    "pickle.dump(x_val_ortho, x_val_file_ortho)\n",
    "x_val_file_ortho.close()\n",
    "\n",
    "x_val_file_ipa = open('data/x_val_ipa.pkl','wb')\n",
    "pickle.dump(x_val_ipa, x_val_file_ipa)\n",
    "x_val_file_ipa.close()\n",
    "\n",
    "y_val_file = open('data/y_val.pkl','wb')\n",
    "pickle.dump(y_val, y_val_file)\n",
    "y_val_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf950826-273f-46d4-9a3a-610e75fa96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(max_encoder_len_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24c4f920-e534-4e15-8a88-9accdd0c123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28132\n",
      "28132\n",
      "28132\n"
     ]
    }
   ],
   "source": [
    "print(len(x_val_ortho))\n",
    "print(len(x_val_ipa))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d525a405-1970-45c6-b18f-997fa26902bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "max_encoder_len_ipa = len(max(filtered_ipa, key=len))\n",
    "print(len(max(x_val_ipa, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bb997-34d9-45be-bcb1-18a08e75bded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
